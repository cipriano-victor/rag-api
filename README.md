# Build a RAG API with FastAPI

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_g3h4i5j6)

---

This project shows a built RAG API from scratch. This was made to learn about RAG (Retrieval-Augmented Generation) works and how to integrate its usage with a web API, using key concepts learnt as web API, frameworks, RAG, embeddings and LLM.

---

## Tools used: Python and Ollama
In this step, Python and Ollama are set up. Python is the programming language used. Ollama is a tool to run AI models. These tools are needed because they give an easy path to develop a web API and to get the integration with an LLM to manage responses.

### Verifying Python and Ollama are working as expected
``` bash 
python3 --version # Verify Python is installed
ollama --version # Verify Ollama is installed
curl http://localhost:11434 # Verify Ollama is running
ollama run tinyllama # Verify tinyllama model is pulled and working
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_i9j0k1l2)

We used Ollama as a tool to run AI models, and used the tinyllama model as a lightweight model who was also good enough in expected performance. You could use lighter models if needed. The model will help the RAG API by using the context to generate a response to the request made by the user.

---

## Setting Up a Python Workspace

It is recommended to set up a Python Workspace as an easy way to setup an environment (files/dependencies) without getting conflicts between system package versions previously installed. Also is easier to eliminate it when you don't need it anymore.

### Virtual environment

A virtual environment is an isolated Python environment that keeps your project's dependencies separate from other Python projects on your computer. In this way you make sure anything installed or run with Python only affects this project. 

``` bash 
python3 -m venv venv # Create a virtual environment
source ./venv/bin/activate # Activate the virtual environment
```

### Dependencies

The packages installed are FastAPI, Chroma, Uvicorn and Ollama. FastAPI is used as a web framework that helps us build APIs quickly. Chroma is used as a vector database that stores document embeddings (numerical representations of text). Uvicorn is used for running our FastAPI app and make it accessible locally on your computer. Ollama library is used to let the code talk to Ollama, giving the API a way to send questions to tinyllama and get responses programmatically.

``` bash 
pip --version # Verify pip is installed
pip install -r requirements.txt # Install dependencies
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_u1v2w3x4)

---

## Setting Up a Knowledge Base

Creating a [knowledge base](/k8s.txt) is the key of this project. A knowledge base is a set of documents used to give the LLM a context to generate an answer to the request made by the user. The RAG API needs to "Retrieve" some base data to build this context for the answer.

### Knowledge base setup
``` bash 
python3 embed_docs.py # Run the embedding script
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_t1u2v3w4)

### Embeddings created

Embeddings are a numerical representation for text and documents that capture meaning. It allows to search embeddings semantically, and not just by matching keywords, but by finding text with similar meaning. The db/ folder contains your knowledge base's embeddings so Chroma can quickly search through them when your API is running. This is important for RAG because Chroma converts your question into an embedding, searches for the most similar embeddings in its database, and returns the matching text. 

---

### How the RAG API works

The RAG API workflow is: given a received request in the question endpoint, the app searchs throught the knowledge base using Chroma to find text that matches the question's meaning, returns the most relevant information from your documents (context) and the question and the context are sent together to tinyllama to generate an answer.

## Testing the RAG API

### Running the FastAPI server
To run the FastAPI server locally, use Uvicorn with the following command:

``` bash 
OLLAMA_HOST='' uvicorn app:app --reload
```

### API query breakdown

``` bash 
curl -X POST "http://127.0.0.1:8000/query" -G --data-urlencode "q=What is Kubernetes?"
```

The command uses the POST method, which is an HTTP method used to send data to a server. The API responds with an answer generated by tinyllama using the context from the knowledge base and the question made.

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_g3h4i5j6)

### Swagger UI exploration

Swagger UI is a documentation page for your FastAPI server. You can use it to explore the API's endpoints, see what parameters they accept, and even try them out right from the browser. The best part about using Swagger UI was that it is automatically generated and also interactive. 

To access Swagger UI, with Uvicorn running your program locally, open your browser and go to:

```http://127.0.0.1:8000/docs```

---

## Adding Dynamic Content
### Adding the /add endpoint

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_w9x0y1z2)

### Dynamic content endpoint working

The /add endpoint allows to build a dynamic knowledge base. This is useful because it allows to accept new content through an API, store it automatically in Chroma and make it searchable immediately without manual file editing or server restarts.

## Containerize a RAG API with Docker

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-docker_x7y8z9a0)

It was added a way to containerize the RAG API. In this case we'll be using Docker, a tool that allows to solve the 'it-works-on-my-machine' problem by packaging the program (in this case our AI API) and everything it needs into a container that runs identically anywhere.

Containerizing means building a Docker image that packages up our RAG API and all of its dependencies. In order to reach this we need a Dockerfile that will contain all the instructions for building the image.

### How the Dockerfile works

A [Dockerfile](/Dockerfile) is a file that contains key instructions to build a Docker image. The ones used in this project are: FROM, COPY, RUN, CMD and more. FROM pulls a base image from Docker Hub, telling Docker to start with a pre-built image with something installed (in this case a light version of Python 3.11). COPY is used to copy the application files from your computer into the container. These files become part of the image. RUN executes commands during build time. CMD defines what command runs when the container starts.

### Building the Docker image
To build the Docker image, run the following command in the terminal from the directory where the Dockerfile is located:

``` bash
docker build -t rag-app .
```

### Running the Docker container
To run the Docker container, use the following command:

``` bash
docker run -p 8000:8000 rag-app
```

### Pulling from Docker Hub
Pulling an image from Docker Hub means downloading an image that's been uploaded via Docker Hub. In this case you don't need to build the image locally. When we ran docker pull, Docker installs a local copy of the exact image previously uploaded. The difference between building locally and pulling from Docker Hub is that we can easily switch computers and still be able to run the containerized API.

The image can be pulled from Docker Hub using the following command:

``` bash
docker pull vicdc21/rag-app
``` 

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-docker_f5g6h7i8)

## Using Docker Compose
Docker Compose is a tool that helps define and run multi-container Docker applications. In this case, we can use it to manage our RAG API container more easily. With Docker Compose, we can define our container's configuration in a single file (docker-compose.yaml) and start or stop the container with simple commands.

You can start the RAG API container using Docker Compose with the following command:

``` bash
docker-compose up -d
```

By running this command, Docker Compose reads the docker-compose.yaml file, builds the image if it doesn't exist, builds also the image for the Ollama service (so you don't have to install it locally) and starts the container in detached mode (running in the background). 

## Deploying RAG API to Kubernetes with Minikube
In this step, we're installing Minikube and kubectl. We need these tools to run a local Kubernetes cluster on the computer instead of using a cloud provider or a large infrastructure, and also to talk to Kubernetes clusters via command-line to deploy applications, inspect resources, view logs, and manage your cluster from the terminal.

Minikube will allow us to create a single-node local-running cluster. For this deployment, you need to load the Docker images into Minikube because Kubernetes doesn't create images by itself, Docker does this. Kubernetes manages and scales containers once they're created. Without this step, Kubernetes would not have an image for creating and running containers. RAG API simply would not be running.

## Creating a Service

We need a service between the Pods and the outside world because it provides a stable networking endpoint (i.e. a stable IP address that let us connect to our API). If a service didn't exist, then we'd have to connect to the Pod that's running our containerized API (this connections breaks each time a Pod is restarted by Kubernetes) directly. The selector finds Pods by matching with a label that MUST match the Deployment's Pod labels. The port configuration allows where to send traffic (routes incoming requests to port where your RAG API is listening), how to expose it (type: NodePort) and the Service port (is the port the Service listens on internally). NodePort enables access to the service from outside the cluster.

## How to deploy the RAG API to Kubernetes
1. Check if Minikube and kubectl are installed.
``` bash 
minikube version
kubectl version --client
```

2. Start Minikube.
``` bash
minikube start
```

3. Apply the Kubernetes configuration files.
``` bash
kubectl apply -f ollama.yaml -f rag.yaml
```

4. Verify that the Pods and Services are running.
``` bash
kubectl get pods # All pods should be in Running status
kubectl get services
```

5. Access the RAG API using Minikube's IP and the NodePort assigned to the service. This command retrieves the URL to access the RAG API service. 
``` bash
minikube service rag-app-service --url
```

## Scaling with Multiple Documents

The embed_docs.py script handles all .txt files in the docs folder to extend the knowledge base. This structure supports growth by letting add how many .txt files you want to the knowledge base.

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-githubactions_g5h6i7j8)

## Creating Semantic Tests

Semantic Tests work to verify the RAG system returns answers with the right meaning, not just the right format. Unlike unit tests that check code logic, semantic tests validate also data quality. These tests ensure quality by verifying the answer includes key concepts as expected.

### Non-deterministic output observation

If we ran the query multiple times using the LLM, we should notice at any point there is a non-deterministic LLM behavior, so tests might pass when they should fail, or fail when they should pass, just because the LLM generated a different response. This is a problem because you can't automate these tests with non-deterministic behavior. For CI/CD to work reliably, we need a solution locally before building automation.

---

## Adding Mock LLM Mode

We're adding mock LLM mode to test whether the right information was found, without the LLM adding variability. This solves the non-determinism problem by returning the retrieved context directly (Same query → Same retrieved document → Same response every time). Reliable testing requires a deterministic output.
 
## Run the tests

In a terminal, run the following command. This command sets the USE_MOCK_LLM environment variable to 1, which activates the mock LLM mode in the application. Then, it starts the FastAPI server using Uvicorn with hot-reloading enabled. We're mocking the LLM to return the retrieved context directly for deterministic testing.
``` bash
USE_MOCK_LLM=1 uvicorn app:app --reload
```

In another terminal, run the semantic tests with the following command:
``` bash
python3 semantic_test.py
```

Note: You can run the tests without the USE_MOCK_LLM variable to see the non-deterministic behavior of the LLM. However, for reliable and repeatable tests, it's recommended to use the mock mode.