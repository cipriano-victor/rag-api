# Build a RAG API with FastAPI

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_g3h4i5j6)

---

This project shows a built RAG API from scratch. This was made to learn about RAG (Retrieval-Augmented Generation) works and how to integrate its usage with a web API, using key concepts learnt as web API, frameworks, RAG, embeddings and LLM.

---

## Tools used: Python and Ollama
In this step, Python and Ollama are set up. Python is the programming language used. Ollama is a tool to run AI models. These tools are needed because they give an easy path to develop a web API and to get the integration with an LLM to manage responses.

### Verifying Python and Ollama are working as expected
``` bash 
python3 --version # Verify Python is installed
ollama --version # Verify Ollama is installed
curl http://localhost:11434 # Verify Ollama is running
ollama run tinyllama # Verify tinyllama model is pulled and working
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_i9j0k1l2)

We used Ollama as a tool to run AI models, and used the tinyllama model as a lightweight model who was also good enough in expected performance. You could use lighter models if needed. The model will help the RAG API by using the context to generate a response to the request made by the user.

---

## Setting Up a Python Workspace

It is recommended to set up a Python Workspace as an easy way to setup an environment (files/dependencies) without getting conflicts between system package versions previously installed. Also is easier to eliminate it when you don't need it anymore.

### Virtual environment

A virtual environment is an isolated Python environment that keeps your project's dependencies separate from other Python projects on your computer. In this way you make sure anything installed or run with Python only affects this project. 

``` bash 
python3 -m venv venv # Create a virtual environment
source ./venv/bin/activate # Activate the virtual environment
```

### Dependencies

The packages installed are FastAPI, Chroma, Uvicorn and Ollama. FastAPI is used as a web framework that helps us build APIs quickly. Chroma is used as a vector database that stores document embeddings (numerical representations of text). Uvicorn is used for running our FastAPI app and make it accessible locally on your computer. Ollama library is used to let the code talk to Ollama, giving the API a way to send questions to tinyllama and get responses programmatically.

``` bash 
pip --version # Verify pip is installed
pip install -r requirements.txt # Install dependencies
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_u1v2w3x4)

---

## Setting Up a Knowledge Base

Creating a knowledge base is the key of this project. A knowledge base is a set of documents used to give the LLM a context to generate an answer to the request made by the user. The RAG API needs to "Retrieve" some base data to build this context for the answer.

### Knowledge base setup

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_t1u2v3w4)

### Embeddings created

Embeddings are a numerical representation for text and documents that capture meaning. It allows to search embeddings semantically, and not just by matching keywords, but by finding text with similar meaning. The db/ folder contains your knowledge base's embeddings so Chroma can quickly search through them when your API is running. This is important for RAG because Chroma converts your question into an embedding, searches for the most similar embeddings in its database, and returns the matching text. 

---

### How the RAG API works

The RAG API workflow is: given a received request in the question endpoint, the app searchs throught the knowledge base using Chroma to find text that matches the question's meaning, returns the most relevant information from your documents (context) and the question and the context are sent together to tinyllama to generate an answer.

## Testing the RAG API

### Running the FastAPI server
To run the FastAPI server locally, use Uvicorn with the following command:

``` bash 
uvicorn main:app --reload
```

### API query breakdown

``` bash 
curl -X POST "http://127.0.0.1:8000/query" -G --data-urlencode "q=What is Kubernetes?"
```

The command uses the POST method, which is an HTTP method used to send data to a server. The API responds with an answer generated by tinyllama using the context from the knowledge base and the question made.

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_g3h4i5j6)

### Swagger UI exploration

Swagger UI is a documentation page for your FastAPI server. You can use it to explore the API's endpoints, see what parameters they accept, and even try them out right from the browser. The best part about using Swagger UI was that it is automatically generated and also interactive. 

To access Swagger UI, with Uvicorn running your program locally, open your browser and go to:

```http://127.0.0.1:8000/docs```

---

## Adding Dynamic Content
### Adding the /add endpoint

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_w9x0y1z2)

### Dynamic content endpoint working

The /add endpoint allows to build a dynamic knowledge base. This is useful because it allows to accept new content through an API, store it automatically in Chroma and make it searchable immediately without manual file editing or server restarts.


