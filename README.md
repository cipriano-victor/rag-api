# Build a RAG API with FastAPI

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_g3h4i5j6)

---

This project shows a built RAG API from scratch. This was made to learn about RAG (Retrieval-Augmented Generation) works and how to integrate its usage with a web API, using key concepts learnt as web API, frameworks, RAG, embeddings and LLM.

---

## Tools used: Python and Ollama
In this step, Python and Ollama are set up. Python is the programming language used. Ollama is a tool to run AI models. These tools are needed because they give an easy path to develop a web API and to get the integration with an LLM to manage responses.

### Verifying Python and Ollama are working as expected
``` bash 
python3 --version # Verify Python is installed
ollama --version # Verify Ollama is installed
curl http://localhost:11434 # Verify Ollama is running
ollama run tinyllama # Verify tinyllama model is pulled and working
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_i9j0k1l2)

We used Ollama as a tool to run AI models, and used the tinyllama model as a lightweight model who was also good enough in expected performance. You could use lighter models if needed. The model will help the RAG API by using the context to generate a response to the request made by the user.

---

## Setting Up a Python Workspace

It is recommended to set up a Python Workspace as an easy way to setup an environment (files/dependencies) without getting conflicts between system package versions previously installed. Also is easier to eliminate it when you don't need it anymore.

### Virtual environment

A virtual environment is an isolated Python environment that keeps your project's dependencies separate from other Python projects on your computer. In this way you make sure anything installed or run with Python only affects this project. 

``` bash 
python3 -m venv venv # Create a virtual environment
source ./venv/bin/activate # Activate the virtual environment
```

### Dependencies

The packages installed are FastAPI, Chroma, Uvicorn and Ollama. FastAPI is used as a web framework that helps us build APIs quickly. Chroma is used as a vector database that stores document embeddings (numerical representations of text). Uvicorn is used for running our FastAPI app and make it accessible locally on your computer. Ollama library is used to let the code talk to Ollama, giving the API a way to send questions to tinyllama and get responses programmatically.

``` bash 
pip --version # Verify pip is installed
pip install -r requirements.txt # Install dependencies
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_u1v2w3x4)

---

## Setting Up a Knowledge Base

Creating a [knowledge base](/k8s.txt) is the key of this project. A knowledge base is a set of documents used to give the LLM a context to generate an answer to the request made by the user. The RAG API needs to "Retrieve" some base data to build this context for the answer.

### Knowledge base setup
``` bash 
python3 embed.py
```

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_t1u2v3w4)

### Embeddings created

Embeddings are a numerical representation for text and documents that capture meaning. It allows to search embeddings semantically, and not just by matching keywords, but by finding text with similar meaning. The db/ folder contains your knowledge base's embeddings so Chroma can quickly search through them when your API is running. This is important for RAG because Chroma converts your question into an embedding, searches for the most similar embeddings in its database, and returns the matching text. 

---

### How the RAG API works

The RAG API workflow is: given a received request in the question endpoint, the app searchs throught the knowledge base using Chroma to find text that matches the question's meaning, returns the most relevant information from your documents (context) and the question and the context are sent together to tinyllama to generate an answer.

## Testing the RAG API

### Running the FastAPI server
To run the FastAPI server locally, use Uvicorn with the following command:

``` bash 
uvicorn app:app --reload
```

### API query breakdown

``` bash 
curl -X POST "http://127.0.0.1:8000/query" -G --data-urlencode "q=What is Kubernetes?"
```

The command uses the POST method, which is an HTTP method used to send data to a server. The API responds with an answer generated by tinyllama using the context from the knowledge base and the question made.

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_g3h4i5j6)

### Swagger UI exploration

Swagger UI is a documentation page for your FastAPI server. You can use it to explore the API's endpoints, see what parameters they accept, and even try them out right from the browser. The best part about using Swagger UI was that it is automatically generated and also interactive. 

To access Swagger UI, with Uvicorn running your program locally, open your browser and go to:

```http://127.0.0.1:8000/docs```

---

## Adding Dynamic Content
### Adding the /add endpoint

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-api_w9x0y1z2)

### Dynamic content endpoint working

The /add endpoint allows to build a dynamic knowledge base. This is useful because it allows to accept new content through an API, store it automatically in Chroma and make it searchable immediately without manual file editing or server restarts.

## Containerize a RAG API with Docker

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-docker_x7y8z9a0)

It was added a way to containerize the RAG API. In this case we'll be using Docker, a tool that allows to solve the 'it-works-on-my-machine' problem by packaging the program (in this case our AI API) and everything it needs into a container that runs identically anywhere.

Containerizing means building a Docker image that packages up our RAG API and all of its dependencies. In order to reach this we need a Dockerfile that will contain all the instructions for building the image.

### How the Dockerfile works

A [Dockerfile](/Dockerfile) is a file that contains key instructions to build a Docker image. The ones used in this project are: FROM, COPY, RUN, CMD and more. FROM pulls a base image from Docker Hub, telling Docker to start with a pre-built image with something installed (in this case a light version of Python 3.11). COPY is used to copy the application files from your computer into the container. These files become part of the image. RUN executes commands during build time. CMD defines what command runs when the container starts.

### Building the Docker image
To build the Docker image, run the following command in the terminal from the directory where the Dockerfile is located:

``` bash
docker build -t rag-api .
```

### Running the Docker container
To run the Docker container, use the following command:

``` bash
docker run -p 8000:8000 rag-api
```

### Pulling from Docker Hub
Pulling an image from Docker Hub means downloading an image that's been uploaded via Docker Hub. In this case you don't need to build the image locally. When we ran docker pull, Docker installs a local copy of the exact image previously uploaded. The difference between building locally and pulling from Docker Hub is that we can easily switch computers and still be able to run the containerized API.

The image can be pulled from Docker Hub using the following command:

``` bash
docker pull vicdc21/rag-app
``` 

![Image](http://learn.nextwork.org/motivated_amber_fierce_fox/uploads/ai-devops-docker_f5g6h7i8)
